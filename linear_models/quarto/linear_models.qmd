---
title: "Linear Model Notes"
author: "Jake Baker"
toc: true
number-sections: true
highlight-style: pygments
format: 
  html: 
    html-math-method: katex
filters:
  - ../../_extensions/pandoc-ext/diagram/diagram.lua

diagram:
  cache: false
---

## OLS Basics

::: {#def-standard_normal}
A random vector $\mathbf{z} \in \mathbb{R^n}$ is standard normal **iff** components 
$(\mathbf{z_i})_{i=1}^n$
are independantly identically distributed $\mathcal{N}(\mathbf{0,I_n})$.
:::

::: {.callout-note}
If a vector in standard normal w.r.t one basis then it is standard normal w.r.t any basis.
:::

::: {#def-ols_lm}
## OLS
An ordinary least squares linear model (OLM) provides an estimate $\hat{\beta}$ of unknown coefficent $\beta \in \mathbb{R^{p \times 1}}$ to the problem,

$$
\mathbf{y} = \mathbf{X} \beta + \mathbf{\varepsilon}
$$

that minimises $\| \hat{\varepsilon} \|$ for $\hat{\varepsilon} = \mathbf{y-X \hat{\beta}}$.

Items $\mathbf{y} \in \mathbb{R}^{n \times 1}, \mathbf{X} \in \mathbb{R^{n \times p}}$ are known.

We **assume** that $\dfrac{\varepsilon}{\sigma}$ is
standard normal (@def-standard_normal)
$\Leftrightarrow \varepsilon \sim \mathcal{N}(\mathbf{0, \sigma^2 I_n})$,
where we may not know $\sigma$.
:::

::: {#thm-ols_solution}
## Solution of OLS
Assume that $\mathbf{X}$ has full rank.

The solution of OLS is given by $\hat{\beta} = \mathbf{(X^TX)^{-1}X^Ty}$.
:::


::: {.proof}
Let subspace $\mathbf{U} \leq \mathbb{R^n}$ generated via the span of $\mathbf{X}$ columns. Then by property of orthogonal projection the 
$\mathbf{\hat{y}=X \beta \in U}$
that minimises 
$\| \hat{\varepsilon} \| = \| \mathbf{y-\hat{y}} \|$ is given by $\mathbf{P_U y}$.

Therefore,
$$
\begin{align*}
\forall u \in U,
\langle u, \hat{\varepsilon}\rangle =0 &\implies \mathbf{(y-X \hat{\beta})^TX} = 0
\\
&\Leftrightarrow
\mathbf{y^T X} = \mathbf{\hat{\beta}^T X^TX}
\\
&\Leftrightarrow
\mathbf{X^Ty} = \mathbf{X^T X \hat{\beta}}
\\
&\Leftrightarrow
\mathbf{\hat{\beta}} = \mathbf{(X^TX)^{-1}X^Ty}
\end{align*}
$$
:::


## Large Model vs Submodel

::: {#thm-anova_F_test}
## ANOVA F-test
Let $p=p_0+p_1, \mathbf{X = [X_0, X_1]} \in \mathbb{R}^{n \times p}$ for 
$\mathbf{X_0} \in \mathbb{R}^{n \times p_0}, \mathbf{X_1} \in \mathbb{R}^{n \times p_1}$

Consider the following two hypothesis of a full model vs a submodel,

$$
\begin{align*}
\textbf{H}_0 \text{ (null)}&:
\mathbf{y = X_0 \beta_\mathcal{N} + \varepsilon},
\quad
&& \mathbf{\beta}_\mathcal{N} \in \mathbb{R}^{p_0 \times 1},
\quad
&&&\mathbf{\varepsilon \sim \mathcal{N}(0,I_n)}
\\
\textbf{H}_1 \text{ (full)}&:
\mathbf{y = X \beta_\mathcal{F} + \varepsilon},
\quad
&& \mathbf{\beta}_\mathcal{F} \in \mathbb{R}^{p \times 1},
\quad
&&&\mathbf{\varepsilon \sim \mathcal{N}(0,I_n)}
\end{align*}
$$

Under the null hypothesis the following statistic is F distributed.
$$
\dfrac{\| \mathbf{\hat{\varepsilon}_\mathcal{N} - \hat{\varepsilon}_\mathcal{F}} \| /p_1}{\| \mathbf{\hat{\varepsilon}} \|/(n-p)}
\sim
F_{p_1, n-p}
$$
:::


::: {.proof}
The following picture is helpful:
:::

## Section

Example reference.
See @thm-anova_F_test.